{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files merged successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取三个CSV文件\n",
    "df1 = pd.read_csv(\"/data4/tongshuo/dataset/aptos2019/train_1.csv\")\n",
    "df2 = pd.read_csv(\"/data4/tongshuo/dataset/aptos2019/test.csv\")\n",
    "df3 = pd.read_csv(\"/data4/tongshuo/dataset/aptos2019/valid.csv\")\n",
    "\n",
    "# 合并这三个DataFrame\n",
    "merged_df = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "\n",
    "# 将合并后的数据输出为一个新的CSV文件\n",
    "merged_df.to_csv('/data4/tongshuo/dataset/aptos2019/ten_fold/all.csv', index=False)\n",
    "print(\"CSV files merged successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been split into 10 folds and saved as CSV files.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# 读取原始数据\n",
    "df = pd.read_csv('/data4/tongshuo/dataset/aptos2019/ten_fold/all.csv')\n",
    "\n",
    "# 确保目标列是 'diagnosis'\n",
    "target_col = 'diagnosis'\n",
    "\n",
    "# 初始化 StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# 保存每个折的数据\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(df, df[target_col])):\n",
    "    # 分别获取训练集和验证集\n",
    "    fold_df = df.iloc[val_idx]\n",
    "    \n",
    "    # 输出为CSV文件\n",
    "    fold_df.to_csv(f'/data4/tongshuo/dataset/aptos2019/ten_fold/fold_{fold}.csv', index=True)\n",
    "\n",
    "print(\"Data has been split into 10 folds and saved as CSV files.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### balance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id_code  diagnosis\n",
      "0  2700754f71e9          2\n",
      "1  2608e1dac5b1          2\n",
      "2  0cb14014117d          3\n",
      "3  374535e0adb8          4\n",
      "4  83038ca49b6d          0\n",
      "1758\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load your dataset (assuming it's in a CSV file)\n",
    "df = pd.read_csv('/data4/tongshuo/dataset/aptos2019/ten_fold_balanced/all.csv')\n",
    "\n",
    "# Step 1: Filter the dataset by diagnosis\n",
    "diagnosis_0 = df[df['diagnosis'] == 0]\n",
    "diagnosis_1 = df[df['diagnosis'] == 1]\n",
    "diagnosis_2 = df[df['diagnosis'] == 2]\n",
    "diagnosis_3 = df[df['diagnosis'] == 3]\n",
    "diagnosis_4 = df[df['diagnosis'] == 4]\n",
    "\n",
    "# Step 2: Randomly select 600 samples from diagnosis 0\n",
    "diagnosis_0_selected = diagnosis_0.sample(n=600, random_state=42)\n",
    "\n",
    "# Step 3: Select all samples from diagnosis 1, 3, and 4\n",
    "diagnosis_1_selected = diagnosis_1\n",
    "diagnosis_3_selected = diagnosis_3\n",
    "diagnosis_4_selected = diagnosis_4\n",
    "\n",
    "# Step 4: Randomly select 300 samples from diagnosis 2\n",
    "diagnosis_2_selected = diagnosis_2.sample(n=300, random_state=42)\n",
    "\n",
    "# Step 5: Combine the selected samples\n",
    "balanced_df = pd.concat([diagnosis_0_selected, diagnosis_1_selected, diagnosis_2_selected, diagnosis_3_selected, diagnosis_4_selected])\n",
    "\n",
    "# Optionally, shuffle the final dataset if you want random ordering\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# The resulting balanced dataset\n",
    "print(balanced_df.head())  # Display the first few rows of the balanced dataset\n",
    "print(len(balanced_df))\n",
    "# Save the balanced dataset to a new CSV file\n",
    "balanced_df.to_csv(\"/data4/tongshuo/dataset/aptos2019/ten_fold_balanced/balanced_all.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been split into 10 folds and saved as CSV files.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# 读取原始数据\n",
    "df = pd.read_csv('/data4/tongshuo/dataset/aptos2019/ten_fold_balanced/balanced_all.csv')\n",
    "# 确保目标列是 'diagnosis'\n",
    "target_col = 'diagnosis'\n",
    "\n",
    "# 初始化 StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# 保存每个折的数据\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(df, df[target_col])):\n",
    "    # 分别获取训练集和验证集\n",
    "    fold_df = df.iloc[val_idx]\n",
    "    \n",
    "    # 输出为CSV文件\n",
    "    fold_df.to_csv(f'/data4/tongshuo/dataset/aptos2019/ten_fold_balanced/fold_{fold}.csv', index=True)\n",
    "\n",
    "print(\"Data has been split into 10 folds and saved as CSV files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been split into 10 folds and saved as CSV files.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# 读取原始数据\n",
    "df = pd.read_excel(\"/data3/tongshuo/dataset/image/SICAPv2/ten_fold/all.xlsx\")\n",
    "\n",
    "# 确保目标列是 'diagnosis'\n",
    "target_col = 'label'\n",
    "\n",
    "# 初始化 StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# 保存每个折的数据\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(df, df[target_col])):\n",
    "    # 分别获取训练集和验证集\n",
    "    fold_df = df.iloc[val_idx]\n",
    "    \n",
    "    # 输出为CSV文件\n",
    "    fold_df.to_csv(f'/data3/tongshuo/dataset/image/SICAPv2/ten_fold/fold_{fold}.csv', index=True)\n",
    "\n",
    "print(\"Data has been split into 10 folds and saved as CSV files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the data fold: [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[1625, 333, 899, 173, 265]\n",
      "3295\n",
      "['a0d04a19cf40', '2']\n",
      "['27bab1432f61', '2']\n",
      "['b22354b5f94b', '1']\n",
      "['a93f1ea3ff4a', '2']\n",
      "['cf603a9ef2d5', '1']\n",
      "['ab1c20a94f3f', '4']\n",
      "['79d44db3da2d', '4']\n",
      "['0d0b8fc9ab5c', '0']\n",
      "i: 0\n",
      "### torch.Size([8, 3, 224, 224]) torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "from PIL import Image\n",
    "import copy\n",
    "import pandas as pd\n",
    "import csv\n",
    "import random\n",
    "from typing import Tuple, Dict\n",
    "from torch import Tensor\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import InterpolationMode\n",
    "import pickle\n",
    "\n",
    "class APTOSDataset(data_utils.Dataset):\n",
    "\n",
    "    def __init__(self, patch_level, img_root, dataset, transform=None, fold=3):\n",
    "        self.data_list = []\n",
    "        # self.items = []\n",
    "        self.transform = transform\n",
    "        self.patch_level = patch_level\n",
    "        if dataset == 'train':\n",
    "            data_num = [i for i in range(10) if i != fold]\n",
    "        elif dataset == 'valid':\n",
    "            data_num = [fold]\n",
    "        print(\"Use the data fold: {}\".format(data_num))\n",
    "        for i in data_num:\n",
    "            #f = open('data_split/dr/fold_{}.csv'.format(i), \"r\")\n",
    "            f = open('/data4/tongshuo/dataset/aptos2019/ten_fold/fold_{}.csv'.format(i), \"r\")\n",
    "            reader = csv.reader(f)\n",
    "            next(reader)\n",
    "            for row in reader:\n",
    "                self.data_list.append(row[1:])\n",
    "\n",
    "        self.label_list = [int(x[-1]) for x in self.data_list] #提出了所有折中的label\n",
    "        # 提取出0, 1, 2\n",
    "        #self.label_list = [x for x in self.label_list if x in {0, 1, 2}]\n",
    "\n",
    "        if self.patch_level == 1:\n",
    "            self.patch_class = self.get_token_class(fold)\n",
    "\n",
    "        self.label_num = [0, 0, 0, 0, 0]\n",
    "        for each in self.label_list:\n",
    "            self.label_num[each] += 1\n",
    "        print(self.label_num) #[23229, 2199, 4763, 786, 638]\n",
    "        print(len(self.data_list)) #31615\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = copy.deepcopy(self.data_list[idx])\n",
    "        print(item)\n",
    "        img = item[0]\n",
    "        label = int(item[1])\n",
    "        img_path = '/data4/tongshuo/dataset/aptos2019/images/' + img + '.png'\n",
    "        # label = int(item[-1])\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tran = transforms.Compose([\n",
    "            transforms.Resize((256, 256), interpolation=InterpolationMode.BILINEAR),\n",
    "            transforms.RandomResizedCrop(224, scale=(0.08, 1.)),\n",
    "            transforms.RandomApply([\n",
    "                transforms.ColorJitter(0.4, 0.4, 0.2, 0.1)  # not strengthened\n",
    "            ], p=0.8),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    train_data = APTOSDataset(None,None,'train',tran,0)\n",
    "    # loader = torch.utils.data.DataLoader(train_data, batch_size=2, shuffle=True, num_workers=4, drop_last=True)\n",
    "    loader = torch.utils.data.DataLoader(train_data, batch_size=8, shuffle=True, drop_last=True)\n",
    "    for i, (a, b) in enumerate(loader):\n",
    "        print('i:',i)\n",
    "        print('###',a.shape, b.shape)\n",
    "        # 判断失败案例（预测错误的样本）\n",
    "        # if predicted[i] != label[i]:\n",
    "        #     # 记录失败的索引、真实标签和预测标签\n",
    "        #     failed_cases.append((batch_idx * len(label) + i, label[i].item(), predicted[i].item()))\n",
    "        #     # 记录失败案例的图像路径\n",
    "        #     img_path = train_loader.dataset.data_list[batch_idx * len(label) + i][0]  # 获取图像的文件名\n",
    "        #     failed_images.append(img_path)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the data fold: [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[3975, 2000, 4044, 853]\n",
      "10872\n",
      "['17B0031079_Block_Region_6_12_16_xini_30193_yini_65523.jpg', '2']\n",
      "['18B0004346A_Block_Region_1_10_5_xini_28790_yini_68945.jpg', '2']\n",
      "['18B0005718B_Block_Region_8_5_8_xini_30489_yini_15722.jpg', '2']\n",
      "['17B0023909_Block_Region_5_10_23_xini_28384_yini_16198.jpg', '2']\n",
      "['18B0003032A_Block_Region_2_21_30_xini_36859_yini_22888.jpg', '2']\n",
      "['17B0011996_Block_Region_5_10_11_xini_17887_yini_63887.jpg', '1']\n",
      "['16B0006668_Block_Region_3_25_7_xini_61646_yini_93244.jpg', '3']\n",
      "['16B0027040_Block_Region_10_21_14_xini_42926_yini_37492.jpg', '0']\n",
      "i: 0\n",
      "### torch.Size([8, 3, 224, 224]) torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "from PIL import Image\n",
    "import copy\n",
    "import pandas as pd\n",
    "import csv\n",
    "import random\n",
    "from typing import Tuple, Dict\n",
    "from torch import Tensor\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import InterpolationMode\n",
    "import pickle\n",
    "\n",
    "class SICAPv2Dataset(data_utils.Dataset):\n",
    "\n",
    "    def __init__(self, patch_level, img_root, dataset, transform=None, fold=3):\n",
    "        self.data_list = []\n",
    "        # self.items = []\n",
    "        self.transform = transform\n",
    "        self.patch_level = patch_level\n",
    "        if dataset == 'train':\n",
    "            data_num = [i for i in range(10) if i != fold]\n",
    "        elif dataset == 'valid':\n",
    "            data_num = [fold]\n",
    "        print(\"Use the data fold: {}\".format(data_num))\n",
    "        for i in data_num:\n",
    "            f = open('/data3/tongshuo/dataset/image/SICAPv2/ten_fold/fold_{}.csv'.format(i), \"r\")\n",
    "            reader = csv.reader(f)\n",
    "            next(reader)\n",
    "            for row in reader:\n",
    "                self.data_list.append(row[1:])\n",
    "\n",
    "        self.label_list = [int(x[-1]) for x in self.data_list] #提出了所有折中的label\n",
    "        # 提取出0, 1, 2\n",
    "        #self.label_list = [x for x in self.label_list if x in {0, 1, 2}]\n",
    "\n",
    "        if self.patch_level == 1:\n",
    "            self.patch_class = self.get_token_class(fold)\n",
    "\n",
    "        self.label_num = [0, 0, 0, 0]\n",
    "        for each in self.label_list:\n",
    "            self.label_num[each] += 1\n",
    "        print(self.label_num) #[23229, 2199, 4763, 786, 638]\n",
    "        print(len(self.data_list)) #31615\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = copy.deepcopy(self.data_list[idx])\n",
    "        print(item)\n",
    "        img = item[0]\n",
    "        label = int(item[1])\n",
    "        img_path = '/data3/tongshuo/dataset/image/SICAPv2/images/' + img\n",
    "        # label = int(item[-1])\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tran = transforms.Compose([\n",
    "            transforms.Resize((256, 256), interpolation=InterpolationMode.BILINEAR),\n",
    "            transforms.RandomResizedCrop(224, scale=(0.08, 1.)),\n",
    "            transforms.RandomApply([\n",
    "                transforms.ColorJitter(0.4, 0.4, 0.2, 0.1)  # not strengthened\n",
    "            ], p=0.8),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    train_data = SICAPv2Dataset(None,None,'train',tran,0)\n",
    "    # loader = torch.utils.data.DataLoader(train_data, batch_size=2, shuffle=True, num_workers=4, drop_last=True)\n",
    "    loader = torch.utils.data.DataLoader(train_data, batch_size=8, shuffle=True, drop_last=True)\n",
    "    for i, (a, b) in enumerate(loader):\n",
    "        print('i:',i)\n",
    "        print('###',a.shape, b.shape)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12081: 4417, 2222, 4494, 948 -> 2500, 2222, 2500, 948"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          image_name  label\n",
      "0  17B0031877_Block_Region_4_14_14_xini_21274_yin...      2\n",
      "1  18B0001159D_Block_Region_3_7_28_xini_37520_yin...      2\n",
      "2  17B0017506_Block_Region_1_2_3_xini_8727_yini_1...      1\n",
      "3  17B0023909_Block_Region_5_13_17_xini_22240_yin...      2\n",
      "4  18B0004349G_Block_Region_15_0_1_xini_24248_yin...      2\n",
      "8170\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load your dataset (assuming it's in a CSV file)\n",
    "df = pd.read_excel(\"/data3/tongshuo/dataset/image/SICAPv2/ten_fold_balanced/all.xlsx\")\n",
    "\n",
    "# Step 1: Filter the dataset by diagnosis\n",
    "diagnosis_0 = df[df['label'] == 0]\n",
    "diagnosis_1 = df[df['label'] == 1]\n",
    "diagnosis_2 = df[df['label'] == 2]\n",
    "diagnosis_3 = df[df['label'] == 3]\n",
    "\n",
    "# Step 2: Randomly select 600 samples from diagnosis 0\n",
    "diagnosis_0_selected = diagnosis_0.sample(n=2500, random_state=42)\n",
    "\n",
    "# Step 3: Select all samples from diagnosis 1, 3, and 4\n",
    "diagnosis_1_selected = diagnosis_1\n",
    "diagnosis_3_selected = diagnosis_3\n",
    "\n",
    "# Step 4: Randomly select 300 samples from diagnosis 2\n",
    "diagnosis_2_selected = diagnosis_2.sample(n=2500, random_state=42)\n",
    "\n",
    "# Step 5: Combine the selected samples\n",
    "balanced_df = pd.concat([diagnosis_0_selected, diagnosis_1_selected, diagnosis_2_selected, diagnosis_3_selected])\n",
    "\n",
    "# Optionally, shuffle the final dataset if you want random ordering\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# The resulting balanced dataset\n",
    "print(balanced_df.head())  # Display the first few rows of the balanced dataset\n",
    "print(len(balanced_df))\n",
    "# Save the balanced dataset to a new CSV file\n",
    "balanced_df.to_csv(\"/data3/tongshuo/dataset/image/SICAPv2/ten_fold_balanced/balanced_all.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been split into 10 folds and saved as CSV files.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# 读取原始数据\n",
    "df = pd.read_csv('/data3/tongshuo/dataset/image/SICAPv2/ten_fold_balanced/balanced_all.csv')\n",
    "# 确保目标列是 'diagnosis'\n",
    "target_col = 'label'\n",
    "\n",
    "# 初始化 StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# 保存每个折的数据\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(df, df[target_col])):\n",
    "    # 分别获取训练集和验证集\n",
    "    fold_df = df.iloc[val_idx]\n",
    "    \n",
    "    # 输出为CSV文件\n",
    "    fold_df.to_csv(f'/data3/tongshuo/dataset/image/SICAPv2/ten_fold_balanced/fold_{fold}.csv', index=True)\n",
    "\n",
    "print(\"Data has been split into 10 folds and saved as CSV files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          name  label\n",
      "0   37638_left      0\n",
      "1   30483_left      0\n",
      "2  32809_right      0\n",
      "3   10701_left      0\n",
      "4   7817_right      0\n",
      "16024\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load your dataset (assuming it's in a CSV file)\n",
    "df = pd.read_csv(\"/data3/tongshuo/dataset/image/DR_dataset/ten_fold_balanced/all.csv\")\n",
    "\n",
    "# Step 1: Filter the dataset by diagnosis\n",
    "diagnosis_0 = df[df['label'] == 0]\n",
    "diagnosis_1 = df[df['label'] == 1]\n",
    "diagnosis_2 = df[df['label'] == 2]\n",
    "diagnosis_3 = df[df['label'] == 3]\n",
    "diagnosis_4 = df[df['label'] == 4]\n",
    "\n",
    "# Step 2: Randomly select 600 samples from diagnosis 0\n",
    "diagnosis_0_selected = diagnosis_0.sample(n=8000, random_state=42)\n",
    "\n",
    "# Step 3: Select all samples from diagnosis 1, 3, and 4\n",
    "diagnosis_1_selected = diagnosis_1\n",
    "diagnosis_3_selected = diagnosis_3\n",
    "diagnosis_4_selected = diagnosis_4\n",
    "# Step 4: Randomly select 300 samples from diagnosis 2\n",
    "diagnosis_2_selected = diagnosis_2.sample(n=4000, random_state=42)\n",
    "\n",
    "# Step 5: Combine the selected samples\n",
    "balanced_df = pd.concat([diagnosis_0_selected, diagnosis_1_selected, diagnosis_2_selected, diagnosis_3_selected, diagnosis_4_selected])\n",
    "\n",
    "# Optionally, shuffle the final dataset if you want random ordering\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# The resulting balanced dataset\n",
    "print(balanced_df.head())  # Display the first few rows of the balanced dataset\n",
    "print(len(balanced_df))\n",
    "# Save the balanced dataset to a new CSV file\n",
    "balanced_df.to_csv(\"/data3/tongshuo/dataset/image/DR_dataset/ten_fold_balanced/balanced_all.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been split into 10 folds and saved as CSV files.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# 读取原始数据\n",
    "df = pd.read_csv('/data3/tongshuo/dataset/image/DR_dataset/ten_fold_balanced/balanced_all.csv')\n",
    "# 确保目标列是 'diagnosis'\n",
    "target_col = 'label'\n",
    "\n",
    "# 初始化 StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# 保存每个折的数据\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(df, df[target_col])):\n",
    "    # 分别获取训练集和验证集\n",
    "    fold_df = df.iloc[val_idx]\n",
    "    \n",
    "    # 输出为CSV文件\n",
    "    fold_df.to_csv(f'/data3/tongshuo/dataset/image/DR_dataset/ten_fold_balanced/fold_{fold}.csv', index=True)\n",
    "\n",
    "print(\"Data has been split into 10 folds and saved as CSV files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been split into 10 folds and saved as CSV files.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# 读取原始数据\n",
    "df = pd.read_csv(\"/data4/tongshuo/Ordinal_Regression/dataset/Adience/age_merged_data.csv\")\n",
    "# 确保目标列是 'diagnosis'\n",
    "target_col = 'label'\n",
    "\n",
    "# 初始化 StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# 保存每个折的数据\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(df, df[target_col])):\n",
    "    # 分别获取训练集和验证集\n",
    "    fold_df = df.iloc[val_idx]\n",
    "    \n",
    "    # 输出为CSV文件\n",
    "    fold_df.to_csv(f'/data4/tongshuo/Ordinal_Regression/dataset/Adience/fold_{fold}.csv', index=True)\n",
    "\n",
    "print(\"Data has been split into 10 folds and saved as CSV files.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reflect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
